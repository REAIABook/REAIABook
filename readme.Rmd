---
title: "Real Estate Analysis in the Information Age: Techniques for Big Data and Statistical Modeling"
author: "Guide to Using the Code Supplied with the Book"
date: "June 11, 2017"
output: html_document
---

The examples and the case studies presented in this book are conducted in the R computing language.^[R Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL <https://www.R-project.org/>.]  All data and code are provided on this website or are downloaded free of charge from the internet.  The code for each chapter (found at <https://github.com/REAIABook/REAIABook>) can be run to reproduce the examples and analyses that are found throughout the book.    

Before you start, you need to do two things.  

1.  Download R from <https://cran.r-project.org/bin/windows/base/> and install.  
2.  Download RStudio from <https://www.rstudio.com/products/rstudio/download/.> and install.    
Note that RStudio is not necessary but it is a helpful interactive development environment (IDE) that will make working with the provided R code easier.    

Once you have opened this can you move on to the code from the individual chapters. Do note that in many cases, code from one chapter depends on code and analyses performed in a previous chapter.  You should also set aside a directory on your computer where you can save the raw and intermediate data as well as the outputs from the graphing and modeling exercises.  You will need to manually input this directory into the code starting in chapter 5.     

## Introduction

How to get the code from Github. 

## Chapter 1

No code

## Chapter 2

No code

## Chapter 3

In chapter 3, we install the necessary packages that we will use throughout this book. You can either copy the code below directly into the 'console' window of RStudio or you can open the *software.r* file and execute it in its entirety with **cntl-enter**.  If asked to choose a mirror for downloading the packages, any will do but the one closest to you is likely to be fastest.     

```{r eval=FALSE}

  install.packages('tidyverse')
  install.packages('sf')
  install.packages('sp')
  install.packages('maptools')
  install.packages('rgeos')
  install.packages('spgwr')
  install.packages('spdep')
  install.packages('geosphere')
  install.packages('OpenStreetMap')
  install.packages('gstat')
  install.packages('RODBC')
  install.packages('RSQLite')
  install.packages('BMA')
  install.packages('MASS')
  install.packages('car')
  install.packages('lmtest')
  install.packages('plm')
  install.packages('scales')

```

After you have installed the above libraries you can move on to the code for chapter 5. 

## Chapter 4

No Code

## Chapter 5

In chapter 5 we gather data from a variety of sources for the two case studies that will run in parallel to this book. The code below will download and unzip the necessary data to reproduce the case studies used in this book. This data will also be used to create the example plots and tables throughout the book. The user will have to specify a particular directory in which the data will be located.  A code directory (the directory in which you downloaded the code from the Introduction chapter) will also need to be specified.  

Please note that, depending on your internet speed, the initial downloading of the files may take some time. Note that once the raw files are downloaded, the code will recognize their existance and not download them again if you run the code a second time, so long as you do not move the files from their downloaded location. Also, you will need at least 2.5 GB of space^[This can be reduced by deleted the downloaded ZIP files after extraction.] in the directory that you indicate in the **data.dir** parameter below.  

### Gathering the Data

Before executing the code file *gather.R*, the user must specify the directory into which the downloaded data will be stored, unzipped, etc.  In the example below we have used **c:/temp/** as the directory.  You should delete this and use a directory of your choice. 

```{r eval=FALSE}

  data.dir <- 'c:/temp/'

```

Next, we check to see if a sub-directory to store the raw zip files exists.  If not, we create one called *raw_zip_files*. 

```{r eval=FALSE}

  if (!dir.exists(file.path(data.dir, 'raw_zip_files'))){
    dir.create(file.path(data.dir, 'raw_zip_files'))
  }

```

We now check to see if the GIS shapefile of the Seattle Police Department Beat Districts has already been downloaded.  If not, we download it from the internet. 

```{r eval=FALSE}

  # Check if file exists
  if (!file.exists(file.path(data.dir, 'raw_zip_files', 'beats.zip'))){
  
    # If it doesn't, then download
    download.file(url=paste0('https://data.seattle.gov/views/nnxn-434b/files/',
                         '96d998d4-ae20-4ea8-b912-436e68982a0d.zip'), 
                  destfile=file.path(data.dir, 'raw_zip_files', 'beats.zip'))
  }  
 

```

We then check to see if a separate directory for the beats data exists.  If it doesn't we create it.  We then unzip the **beats.zip** file into the new *beats* directory.  

```{r eval=FALSE}

  # Create a directory for beats data if one doesn't exist
  if (!dir.exists(file.path(data.dir, 'beats'))){
    dir.create(file.path(data.dir, 'beats'))
  }
  
  # Unzip the files
  unzip(zipfile=file.path(data.dir, 'raw_zip_files', 'beats.zip'),
        exdir=file.path(data.dir, 'beats'))

```

Next, we  download the property parcel (cadastre) shapefile (if not done so in the past).  We then create a separate sub-directory for the geographic data (if not already present) and then and unzip it into the *geographic* sub-directory.  

```{r eval=FALSE}

  # Check if file exists
  if(!file.exists(file.path(data.dir, 'raw_zip_files', 'parcel_shapefile.zip'))){
    
    # If it doesn't, then download
    download.file(url=paste0('ftp://ftp.kingcounty.gov/gis-web/web/',
                             'GISData/parcel_SHP.zip'), 
                  destfile=file.path(data.dir, 'raw_zip_files', 'parcel_shapefile.zip'))
  }

  # Create a directory if one doesn't exist
  if (!dir.exists(file.path(data.dir, 'geographic'))){
    dir.create(file.path(data.dir, 'geographic'))
  }
  
  # Unzip the files
  unzip(zipfile=file.path(data.dir, 'raw_zip_files', 'parcel_shapefile.zip'), 
        exdir=file.path(data.dir, 'geographic'))

```

Finally, we gather the county assessor data.  This includes sales transactions, information about the land parcel and information about the residential buildings.  Before we download any data we create a sub-directory called *assessor* if one doesn't already exist.  

```{r eval=FALSE}

  # Create a directory if one doesn't exist
  if(!dir.exists(file.path(data.dir, 'assessor'))){
    dir.create(file.path(data.dir, 'assessor'))
  }

```

We start by downloading the sales transaction file, if it does not already exist. Once downloaded, it is unzipped into the *assessor* sub-directory.

```{r eval=FALSE}

  # Check if file exists
  if (!file.exists(file.path(data.dir, 'raw_zip_files', 'sales.zip'))){
    
    # If it doesn't, then download
    download.file(url=paste0('http://your.kingcounty.gov/extranet/assessor/',
                             'Real Property Sales.zip'), 
                  destfile=file.path(data.dir, 'raw_zip_files', 'sales.zip'))
  }
  
  # Unzip
  unzip(zipfile=file.path(data.dir, 'raw_zip_files', 'sales.zip'), 
        exdir=file.path(data.dir, 'assessor'))

```

We then do the same for the parcel and the residential building (resbldg) information. 
  
```{r eval=FALSE}

 ## Parcel Tabular File

  # Check if file exists
  if (!file.exists(file.path(data.dir, 'raw_zip.files', 'parcel.zip'))){
    
    # If it doesn't, then download
    download.file(url='http://your.kingcounty.gov/extranet/assessor/Parcel.zip', 
                  destfile=file.path(data.dir, 'raw_zip_files', 'parcel.zip'))
  }
  
  # Unzip
  unzip(zipfile=file.path(data.dir, 'raw_zip_files', 'parcel.zip'), 
        exdir=file.path(data.dir, 'assessor'))
  
```    
  
```{r eval=FALSE} 

 ## Residential Building File  
  
  # Check if file exists
  if (!file.exists(file.path(data.dir, 'raw_zip_files', 'resbldg.zip'))){
    
    # If it doesn't, then download
    download.file(url=paste0('http://your.kingcounty.gov/extranet/assessor/',
                             'Residential Building.zip'), 
                  destfile=file.path(data.dir, 'raw_zip_files', 'resbldg.zip'))
  }
    
  # Unzip
  unzip(zipfile=file.path(data.dir, 'raw_zip_files', 'resbldg.zip'), 
        exdir=file.path(data.dir, 'assessor'))

```

We then add the crime report statisitics from the City of Seattle, downloading them into the 'crime' directory. 

```{r eval=FALSE} 

  # Create a directory for assessor data if one doesn't exist
  if (!dir.exists(file.path(data.dir, 'crime'))){
    dir.create(file.path(data.dir, 'crime'))
  }
  
  # Download .csv from the City of Seattle
  download.file(url=paste0('https://data.seattle.gov/api/views/7ais-f98f/rows.csv?',
                           'accessType=DOWNLOAD'), 
                destfile=file.path(data.dir, 'crime', 'seattle_crime.csv' ))

```

And finally, we download the pre-analyzed twitter sentiment data from the REAIABook Github site and place in the 'tweets' directory. 

```{r eval=FALSE} 

  # Create a directory for assessor data if one doesn't exist
  if (!dir.exists(file.path(data.dir, 'tweets'))){
    dir.create(file.path(data.dir, 'tweets'))
  }
  
  # Download .csv from the City of Seattle
  download.file(url=paste0('http://raw.githubusercontent.com/REAIABook/',
                           'REAIABook/master/tweetSentiment.csv'), 
                destfile=file.path(data.dir, 'tweets', 'sentimenttweets.csv' ))

```

## Chapter 6

Chapter 6 discusses data management.  In the code below we move the raw, downloaded tabular data into a database format, adding unique identifiers in the process.  For the geo-spatial data we convert the shapefiles to an R data objects and re-project the coordinates to match a standard coordinate reference system format. 

We begin the process by loading a number of packages or libraries that will be needed to complete this process.  These packages were installed on your machine with the code from Chapter 3 above.  These packages are developed by third-party users to augment the R language and represent one of the great benefits of using R.  

```{r eval=FALSE}

  library(RSQLite)
  library(RODBC)
  library(tidyverse)
  library(maptools)
  library(sf)
  library(sp)

```

Next, the user must enter the directory where the data is stored, **data.dir** and the directory where the code has been downloaded, **code.dir**. Both directories should be identical to the directories entered in Chapter 5. 

```{r eval=FALSE}

  data.dir <- 'c:/temp/'             # For Example
  code.dir <- 'c:/code/REAIA_Book/'  # For Example

```

We load in a set of custom functions that have been developed to help with the data integration and cleaning process. 

```{r eval=FALSE}

 source(file.path(code.dir, 'custom_functions.R'))

```

We also create a name and path for the database into which we'll integrate all of the data. 

```{r eval=FALSE}

  data.db <- file.path(data.dir, 'seattleCaseStudy.db')

```

### Manage Assessors data

To better manage the various assessor's data files -- the sales, parcel and residential building information -- we combine them into a single SQLite database.  This also allows for greater portability of the files.  This same database will contain the various iterations of the cleaned sales data as well.  The **convertCSVtoSQLite()** function is a custom function that we have developed for the task of combining various CSV files into a single SQLite database. If you don't want to see the progress of the process you can change the *verbose* argument to FALSE. 

```{r eval=FALSE}
 
 ## Convert CSVs to SQLite

 if (!file.exists(data.db)){
  
   convertCSVtoSQLite(dataPathCurrent=file.path(data.dir, 'assessor'),
                      dataPathNew=data.dir,
                      newFileName='seattleCaseStudy.db',
                      fileNames=c('EXTR_RPSale.csv',
                                  'EXTR_Parcel.csv',
                                  'EXTR_Resbldg.csv'),
                      tableNames=c('Sales',
                                     'Parcel',
                                     'ResBldg'),
                      overWrite=TRUE,
                      verbose=TRUE,
                      writeRowNames=FALSE)
  }

```

#### Sales data unique identifiers

In this section we create a set of unique identifiers for the sales data.  

We start by opening a connection to the SQL database that we created above and reading in the raw sales information.

```{r eval=FALSE}

  # Set connection
  db.conn <- dbConnect(dbDriver('SQLite'), data.db)
  
  # Read in sales data
  sales.data <- dbReadTable(db.conn, 'Sales')

```

We then use a multiple step process to create a unique identifer field, based on the excise tax affidavit number.  We begin by creating a new field called **UID** that contains the excise tax number.  We then re-order the columns to put this value on the far left and then sort the data by the **UID** field.

```{r eval=FALSE}

  # Make new UID from ExciseTaxNbr
  sales.data <- dplyr::mutate(sales.data, UID=ExciseTaxNbr)
  
  # Place Excise Tax Number on far left
  sales.data <- dplyr::select(sales.data, UID, everything())

  # Order by that field
  sales.data <- dplyr::arrange(sales.data, UID)

```  
  
As some of these numbers are duplicates due to multiple parcel (or property) sales, we must do a little more work to make this a unique identifier.  First, we create a separate data frame of those excise tax numbers that appear more than once.  

```{r eval=FALSE}
  
  uid.mult <- sales.data %>% 
    dplyr::group_by(UID) %>%
      dplyr::summarize(rec.count=n()) %>%
       dplyr::filter(rec.count > 1)
  
```

Second, we create a vector showing the count or number of times each of the duplicated UIDs appear.

```{r eval=FALSE}

  # Develop a list full of proper suffixes
  uid.suffix <- lapply(split(uid.mult, uid.mult$UID), 
                       function(x) 1:(x$rec.count))
  names(uid.suffix) <- NULL
  
```

We then add these counts (suffix) to the sales data.  Those with a UID that is unique receive a 0. Finally, we add the suffix value to the original UID to make an actual unique identifier.  The suffix field is removed. 

```{r eval=FALSE}

  # Apply suffix to sales.data
  sales.data$uid.suffix <- 0
  sales.data$uid.suffix[sales.data$UID %in% uid.mult$UID] <- unlist(uid.suffix)
  
  # Concatenate to make an unique ID
  sales.data$UID <- ifelse(sales.data$uid.suffix == 0,
                           sales.data$UID,
                           paste0(sales.data$UID, '..', sales.data$uid.suffix))
  
  # Remove uid.suffix field
  sales.data$uid.suffix <- NULL

```

We write this data back to the database, removing the old table first. 
  
```{r eval=FALSE}
  
  dbRemoveTable(db.conn, 'Sales')
  dbWriteTable(db.conn, 'Sales', sales.data, row.names=FALSE)
  
```

#### Parcel and res bldg data unique identifiers

Here we add unique identifiers to the King County Assessors parcel and residential building tabular data.  The unique identifiers (**pinx**) are simply an extension of the County's parcel identification numbers (PINs) where we add two leading '.'s in order to avoid dropped leading zeros in future uses. A custom function call **buildPinx()** is used to accomplish this.  Full code for the **buildPinx()** function can be found in the *custom_functions.R* file.

We start by reading the raw parcel tabular data, creating the **pinx** unique identifier and then writing it back to the database.  

```{r eval=FALSE}

  # Read in data
  parcel.data <- dbReadTable(db.conn, 'Parcel')
  
  # Add unique pinx field
  parcel.data <- buildPinx(parcel.data)
  
  # Write out
  dbRemoveTable(db.conn, 'Parcel')
  dbWriteTable(db.conn, 'Parcel', parcel.data, row.names=FALSE)
  
```

We then do the same with the residential building tabular data. As some properties have more than one residential building a suffix is added here to indicate which building it is.  

```{r eval=FALSE}
  
  # Read in data
  resbldg.data <- dbReadTable(db.conn, 'ResBldg')
  
  # Add unique pinx field
  resbldg.data <- buildPinx(resbldg.data)
  
  # Add the bldgnbr to complete unique id
  resbldg.data$pinx <- paste0(resbldg.data$pinx, '.', resbldg.data$BldgNbr)
  names(resbldg.data)[1] <- 'BldgID'
  
  # Write out
  dbRemoveTable(db.conn, 'ResBldg')
  dbWriteTable(db.conn, 'ResBldg', resbldg.data, row.names=FALSE)

```

As we have loaded a number of large datasets, we clean our our working RAM here.

```{r eval=FALSE}

  rm(resbldg.data); rm(parcel.data); rm(sales.data)
  gc()

```

#### Parcel Shapefile conversion

We now load in the parcel shapefile and convert it from a polygon to a point coverage.  We also change the coordinate reference system.  

We start by loading the parcel shapefile from the King County GIS department into R as a simple features object (sf).

```{r eval=FALSE}
  
  parcels <- st_read(file.path(data.dir, 'geographic', 'parcel', 'parcel.shp'), 
                     quiet=TRUE)

```

We then transform the coordinate reference system to match the other spatial data we will use later. 

```{r eval=FALSE}

  parcels <- st_transform(parcels, 4326)
  
```

We then extract the centroid of each parcel polygon and conver this to a simple features data frame.  We add the parcel identification number to each point as the unique identifier. 

```{r eval=FALSE}

  # Extract centroid Lat longs
  parcel.centroids <- st_centroid(parcels)
  
  # Convert to a simple features data.frame
  parcel.centroids <- st_sf(parcel.centroids)
  
  # Add PIN values
  parcel.centroids$PIN <- parcels$PIN
  
```

Finally, we save this object as an .Rdata file for faster loading in the future. 

```{r eval=FALSE}
  
  save(parcel.centroids, file=file.path(data.dir, 'geographic', 'parcelcentroids.Rdata'))

```

#### Add in Crime Beat Data

Now we load in the crime data and add it to the database.  

First we load the crime data from the .csv file downloaded from the City of Seattle.

```{r eval=FALSE}

  crime.data <- read.csv(file.path(data.dir, 'crime', 'seattle_crime.csv'))

```
  
We then convert all of the field names to lower case and rename the unique identifier field to a simpler name, uid.  
  
```{r eval=FALSE}  
  
  # Convert to lower case
  names(crime.data) <- tolower(names(crime.data))
  
  # Rename unique identifier
  names(crime.data)[1] <- 'uid'
  
```

We then write these data to the database, removing the existing table if necessary. 
  
```{r eval=FALSE}

  # Check if exists and remove
  if(dbExistsTable(db.conn, 'Crime')){
    dbRemoveTable(db.conn, 'Crime')
  }

  # Write to database
  dbWriteTable(db.conn, 'Crime', crime.data, row.names=FALSE)

```

#### Add Twitter sentiment data to database    

We now add the twitter sentiment data (available from the REAIA Github site) to the database. 

First we load in the sentiment scored tweets directly from the Github site.  

```{r eval=FALSE}  

 # Read in tweet sentiment data  
  tweet.sent <- read.csv(file.path(data.dir, 'tweets', 'sentimenttweets.csv'),
                         header=TRUE)
  
  # Remove if exists 
  if (dbExistsTable(db.conn, 'SentimentTweets')){
    dbRemoveTable(db.conn, 'SentimentTweets')
  }
  
  # Write to database
  dbWriteTable(db.conn, 'SentimentTweets', tweet.sent, row.names=FALSE)

```

#### Police Beat Spatial Data

Finally, we manage the Seattle Police Beat spatial data.

We begin by reading in the police beat boundaries as a simple features polygon object. 

```{r eval=FALSE}  

  beats <- st_read(file.path(data.dir, 'beats', 'SPD_BEATS_WGS84.shp'), 
                   quiet=TRUE)

```

We then transform the coordinate reference system.

```{r eval=FALSE}  

  beats <- st_transform(beats, 4326)

```

As we will work with other spatial packages in later analyses, we also convert simple features (sf) object to a an *sp* object (an earlier form of spatial handling in R). To this, we add the beat numbers as a unique identifier. 
  
```{r eval=FALSE}  
  
  # Convert to sp object
  beats.sp <- as(beats, 'Spatial')

  # Add id and beat numbers
  beats.sp@data$id <- paste0("ID", 1:nrow(beats.sp@data))

```

We then also convert the police beat coverage into a simple data frame format (fortified) that can be plotted in ggplot. This includes adding the beat identification code to the data frame. 
  
```{r eval=FALSE}  

  # Convert to a fortified object (data.frame)
  beats.spf <- broom::tidy(beats.sp)
  
  # Add beat id
  beats.spf$beat <- beats.sp@data$beat[match(beats.spf$id, 
                                             beats.sp@data$id)]
  
```

Finally, we save all three object types of the beat polygon data to an r workspace for easier loading later on. 

```{r eval=FALSE}  

  save(beats, beats.sp, beats.spf, 
       file= file.path(data.dir, 'geographic', 'beats.Rdata'))

```

We finish the manage stage of our analysis by closing the connection to the database. 

```{r eval=FALSE}  

  dbDisconnect(db.conn)

```





